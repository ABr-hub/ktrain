{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Learning Rates with *ktrain*\n",
    "\n",
    "Neural networks have many hyperparameters that need to be set before training begins. While, in practice, many hyperparameters have fairly reasonable defaults (e.g., ReLu activation, Xavier initialization, a kernel size of 3 in Convolutional Neural Networks), some do not and should be tuned. One of these is the learning rate, which governs the degree to which weights are adjusted during training.  Even after arriving at a good initial learning rate, it has been shown that varying the learning rate during training is effective in helping to minimize loss and improve generalization. *ktrain* provides a number of built-in methods to make it easy to tune and adjust learning rates to more effectively minimize loss during training.\n",
    "\n",
    "To demonstrate these capabilities, we will begin by loading some text data into NumPy arrays and defining a simple text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; \n",
    "import ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  and prepare data as you normally would in Keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "NUM_WORDS = 20000\n",
    "MAXLEN = 400\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=MAXLEN)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model as you normally would in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, 50, input_length=MAXLEN))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use *ktrain*, one simply wraps the model and the data in a Learner object using the ```get_learner``` function.  This Learner object will be used to help tune and train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped model and data are both directly accessible.  For instance, the model can be saved and loaded like normal in Keras (e.g,. ```learner.model.save('my_model.h5')```).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Learning Rate Finder\n",
    "\n",
    "The Learner object can be used to find the best learning rate for your model.  First, we use ```lr_find``` to track the loss as the learning rate is increased and then use ```lr_plot``` to identify the maximal learning rate associated with a falling loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 5s 196us/step - loss: 0.6932 - acc: 0.4902\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 4s 178us/step - loss: 0.6926 - acc: 0.5456\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 4s 177us/step - loss: 0.5968 - acc: 0.7288\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 4s 174us/step - loss: 0.3167 - acc: 0.8695\n",
      "Epoch 5/5\n",
      " 7840/25000 [========>.....................] - ETA: 3s - loss: 0.5984 - acc: 0.8476\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like the maximal learning rate associated with a still-falling loss (prior the loss diverging).  Based on the plot, we will start with a learning rate of 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Training\n",
    "\n",
    "It is sometimes advantageous to train interactively.  For instance, one can train a model for one or two epochs using one learning rate.  Then, based on the results, a higher or lower learning rate can be used for subsequent epochs.  *ktrain* makes such interactive training easy.  Here, using the fit method of the Learner object, we train a single epoch at the learning rate found previously and a second epoch at a slightly lower learning rate.  The first argument is the learning rate and the second argument is the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 5s 199us/step - loss: 0.4007 - acc: 0.8258 - val_loss: 0.3452 - val_acc: 0.8436\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 5s 181us/step - loss: 0.2122 - acc: 0.9278 - val_loss: 0.2864 - val_acc: 0.8874\n"
     ]
    }
   ],
   "source": [
    "# reinitialize the model to train from scratch \n",
    "learner.set_model(get_model())\n",
    "\n",
    "hist = learner.fit(0.005, 1)\n",
    "hist = learner.fit(0.0005, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Learning Rate Schedules\n",
    "\n",
    "In the example above, a static learning rate is used throughout each epoch.  It is sometimes beneficial to employ the use of learning rate schedules to *automatically* adjust the learning rate during the course training to more effectively minimize loss. Such adjustments can help jump out of suboptimal areas in the loss landscape and get to \"sweet spots\" with minimal loss that generalize well.  *ktrain* allows you to easily employ a variety of demonstrably effective learning rate policies during training. These include:\n",
    "\n",
    "* a [triangular learning rate policy](https://arxiv.org/abs/1506.01186) available via the ```autofit``` method\n",
    "* a [1cycle policy](https://arxiv.org/abs/1803.09820) available via the ```fit_onecycle``` method\n",
    "* an [SGDR](https://arxiv.org/abs/1608.03983) (Stochastic Gradient Descent with Restart) schedule available using the ```fit``` method by supplying a *cycle_len* argument.\n",
    "\n",
    "\n",
    "### SGDR\n",
    "We will begin by covering SGDR. *ktrain* allows you to easily employ an SGDR learning rate policy in a similar style to that of the *fastai* library. We will begin with covering the cycle_len parameter.\n",
    "\n",
    "**cycle_len:** When *cycle_len* is not None, the second argument fo ```fit``` is interpreted as the number of cycles instead of the number of epochs.  For instance, the following call runs 2 cycles each of length 2 epochs - totaling 4 (or 2 * 2) epochs.  The learning rate gradually decreases throughout the 2-epoch cycle and then restarts at 5e-3 at the start of a new 2-epoch cycle.  Decreases follow a functional form (cosine annealing). More information can be found in the original [SGDR paper](https://arxiv.org/abs/1608.03983)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 226us/step - loss: 0.4060 - acc: 0.8299 - val_loss: 0.3057 - val_acc: 0.8817\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 216us/step - loss: 0.2251 - acc: 0.9224 - val_loss: 0.2878 - val_acc: 0.8870\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 216us/step - loss: 0.2049 - acc: 0.9236 - val_loss: 0.2920 - val_acc: 0.8822\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 219us/step - loss: 0.1397 - acc: 0.9544 - val_loss: 0.2826 - val_acc: 0.8892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82bd340fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reinitialize the model to train from scratch \n",
    "learner.set_model(get_model())\n",
    "\n",
    "# training using cycle_len \n",
    "learner.fit(5e-3, 2, cycle_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cycle_mult:** The cycle_mult parameter allows you to increase the cycle length as training progresses.  For instance, cycle_mult=2 will double the length of the cycle.  In the example below, seven epochs are run:\n",
    "- first cycle has length of one epoch\n",
    "- second cycle has length two epochs\n",
    "- third cycle has length of four epochs\n",
    "Each cycle will begin at a learning rate of 5e-3 and gradually decrease until it resets at the beginning of the next cycle.\n",
    "\n",
    "Note that the example below overfits. It is shown to merely illustrate the *cycle_mult* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/7\n",
      "25000/25000 [==============================] - 6s 225us/step - loss: 0.4237 - acc: 0.8269 - val_loss: 0.3427 - val_acc: 0.8723\n",
      "Epoch 2/7\n",
      "25000/25000 [==============================] - 6s 222us/step - loss: 0.2596 - acc: 0.9006 - val_loss: 0.2818 - val_acc: 0.8884\n",
      "Epoch 3/7\n",
      "25000/25000 [==============================] - 5s 218us/step - loss: 0.1744 - acc: 0.9430 - val_loss: 0.2772 - val_acc: 0.8903\n",
      "Epoch 4/7\n",
      "25000/25000 [==============================] - 6s 222us/step - loss: 0.1707 - acc: 0.9376 - val_loss: 0.2958 - val_acc: 0.8848\n",
      "Epoch 5/7\n",
      "25000/25000 [==============================] - 6s 221us/step - loss: 0.1146 - acc: 0.9620 - val_loss: 0.3238 - val_acc: 0.8792\n",
      "Epoch 6/7\n",
      "25000/25000 [==============================] - 6s 221us/step - loss: 0.0821 - acc: 0.9762 - val_loss: 0.3391 - val_acc: 0.8797\n",
      "Epoch 7/7\n",
      "25000/25000 [==============================] - 6s 221us/step - loss: 0.0683 - acc: 0.9818 - val_loss: 0.3408 - val_acc: 0.8797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82bbae1ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuild the model to train from scratch \n",
    "learner.set_model(get_model())\n",
    "\n",
    "# training using cycle_len \n",
    "learner.fit(5e-3, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangular Learning Rate Policy via ```autofit```\n",
    "\n",
    "The ```autofit``` method in *ktrain* employs a default cyclical learning rate schedule that tends to work well in practice.  The default learning rate schedule in ```autofit``` is currently the [triangular learning rate policy](https://arxiv.org/abs/1506.01186).\n",
    "\n",
    "The ```autofit``` method accepts two primary arguments. The first (required) is the learning rate (**lr**) to be used, which can be found using the learning rate finder above. The second is optional and indicates the number of epochs (**epochs**) to train.  If **epochs** is not supplied as a second argument, then ```autofit``` will train until the validation loss no longer improves after a certain period. This period can be configured using the **early_stopping** argument.  At the end of training, the weights producing the lowest validation loss are automatically loaded into the model, when **early_stopping** is enabled.  The ```autofit``` method can also automatically reduce the maximum (and base) learning rates in the triangular policy when validation loss no longer improves. This can be configured using the **reduce_on_plateau** and **reduce_factor** arguments to ```autofit```.  \n",
    "\n",
    "Example:\n",
    "```\n",
    "learner.autofit(0.001, 20, reduce_on_plateau=2, reduce_factor=10)\n",
    "```\n",
    "\n",
    "The above will reduce the maximum and base learning rates in the triangular policy by a factor of 10 after two consecutive epochs of no improvement in validation loss.  Validation loss (i.e., val_loss) is the default criterion for both **early_stopping** and **reduce_on_plateau**.  To use validation accuracy instead, use invoke ```autofit``` with ```monitor='val_acc'```.\n",
    "\n",
    "Here, we will use the ```autofit``` method and run the main training phase for two epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.005...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 6s 232us/step - loss: 0.4725 - acc: 0.7838 - val_loss: 0.3253 - val_acc: 0.8768\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 6s 231us/step - loss: 0.2539 - acc: 0.9066 - val_loss: 0.2838 - val_acc: 0.8879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82b2643358>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuild the model to train from scratch \n",
    "learner.set_model(get_model())\n",
    "\n",
    "# training using autofit\n",
    "learner.autofit(0.005, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cooldowns\n",
    "Since we are not [overfitting](https://en.wikipedia.org/wiki/Overfitting#Machine_learning) yet (i.e., validation loss is not increasing while training loss decreases), let's do a few more \"cooldowns\" starting at a smaller learning rate to improve the accuracy score further using the regular ```fit``` method that employs SGDR. These \"cooldown\" epochs will start the learning rate at 0.005/10 and gradually decrease it to a very small value.  We will use the **checkpoint_folder** argument covered earlier, so that we can restore the weights from any epoch in case we train too much and overfit.  If you are not using Linux, you should set this to your folder path of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 6s 226us/step - loss: 0.1932 - acc: 0.9353 - val_loss: 0.2812 - val_acc: 0.8881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82a07025c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.005/10, 1, cycle_len=1, checkpoint_folder='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 6s 225us/step - loss: 0.1874 - acc: 0.9373 - val_loss: 0.2796 - val_acc: 0.8891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82507b6da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.005/10, 1, cycle_len=1, checkpoint_folder='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 6s 226us/step - loss: 0.1817 - acc: 0.9385 - val_loss: 0.2785 - val_acc: 0.8894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82507b65c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.005/10, 1, cycle_len=1, checkpoint_folder='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 6s 230us/step - loss: 0.1565 - acc: 0.9485 - val_loss: 0.2779 - val_acc: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82b1e5c9e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.005/20, 1, cycle_len=1, checkpoint_folder='/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are running multiple short cooldown phases here - cycles of only one epoch. This essentially amounts to SGDR.  Although we are not doing it here, we can also run one longer cooldown by simply calling ```fit``` with a larger value for ```cycle_len``` and leaving the number of cycles at 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 1cycle Policy\n",
    "\n",
    "The [1cycle policy](https://arxiv.org/pdf/1803.09820.pdf) was proposed by Leslie Smith (as was the triangular learning rate policy).  The 1cycle policy runs a single triangular cycle over the course of training and then annihilates the learning rate to a near-zero value towards the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 0.005...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 6s 237us/step - loss: 0.5577 - acc: 0.7512 - val_loss: 0.3761 - val_acc: 0.8528\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 6s 233us/step - loss: 0.2707 - acc: 0.8985 - val_loss: 0.2880 - val_acc: 0.8841\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 6s 233us/step - loss: 0.1755 - acc: 0.9400 - val_loss: 0.2774 - val_acc: 0.8909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8204771ba8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuild the model to train from scratch \n",
    "learner.set_model(get_model())\n",
    "\n",
    "# training using the 1cycle policy\n",
    "learner.fit_onecycle(0.005, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy here is **~89%** using only unigram feaures and a simple model. In the [text classification notebook](https://github.com/amaiya/ktrain/blob/master/tutorial-04-text-classification.ipynb), we show that an accuracy of **~92.3%** can be acheived on this dataset in mere seconds using built-in convenience methods in *ktrain*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
