{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "Sequence tagging (or sequence labeling) involves classifying words or sequences of words as representing some category or concept of interest.  One example of sequence tagging is Named Entity Recognition (NER), where we classify words or sequences of words that identify some entity such as a person, organization, or location.  In this tutorial, we will show how to use *ktrain* to perform sequence tagging in three simple steps.\n",
    "\n",
    "## STEP 1: Load and Preprocess Data\n",
    "\n",
    "The `entities_from_txt` function can be used to load tagged sentences from a text file.  The text file can be in one of two different formats: 1) the [CoNLL2003 format](https://www.aclweb.org/anthology/W03-0419) or 2) the [Groningen Meaning Bank (GMB) format](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). In both formats, there is one word and its associated tag on each line (where the word and tag are delimited by a space, tab or comma).  Words are ordered as they appear in the sentence.  In the CoNLL2003 format, there is a blank line that delineates sentences.  In the GMB format, there is a third column for Sentence ID that assignes a number to each row indicating the sentence to which the word belongs.  If you are building a sequence tagger for your own use case, the training data should be formatted into one of these two formats.\n",
    "\n",
    "In this notebook, we will be building a sequence tagger using the Groningen Meaning Bank NER dataset available on Kaggle [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). The format essentially looks like this (with fields being delimited by comma):\n",
    "```\n",
    "      SentenceID   Word     Tag    \n",
    "      1            Paul     B-PER\n",
    "      1            Newman   I-PER\n",
    "      1            is       O\n",
    "      1            a        O\n",
    "      1            great    O\n",
    "      1            actor    O\n",
    "      1            .        O\n",
    " ```\n",
    "\n",
    "We will be using the file `ner_dataset.csv` (which conforms to the format above) and will load and preprocess it using the `entities_from_txt` function.  The output is simlar to data-loading functions used in previous tutorials and includes the processed training set, processed validaton set, and an instance of `NERPreprocessor`.  \n",
    "\n",
    "The Kaggle dataset `ner_dataset.csv` the three columns of interest (mentioned above) are labeled 'Sentence #', 'Word', and 'Tag'.  Thus, we specify these in the call to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  47959\n",
      "Number of words in the dataset:  35178\n",
      "Tags: ['B-eve', 'I-art', 'I-org', 'I-geo', 'O', 'B-org', 'B-nat', 'B-gpe', 'B-tim', 'I-eve', 'B-art', 'I-tim', 'B-geo', 'B-per', 'I-gpe', 'I-nat', 'I-per']\n",
      "Number of Labels:  17\n",
      "Longest sentence: 104 words\n"
     ]
    }
   ],
   "source": [
    "DATAFILE = '/home/amaiya/data/groningen_meaning_bank/ner_dataset.csv'\n",
    "(trn, val, preproc) = text.entities_from_txt(DATAFILE,\n",
    "                                             embeddings='word2vec',\n",
    "                                             sentence_column='Sentence #',\n",
    "                                             word_column='Word',\n",
    "                                             tag_column='Tag', \n",
    "                                             data_format='gmb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, notice that we suppied `embeddings='word2vec'`.  This directs *ktrain* to employ pretrained word vectors trained by a Word2Vec continuous-bag-of-words model (CBOW).   The word2vec embeddings are 1.5G and will be automatically downloaded to and loaded in STEP 2 (download location is `<home_directory>/ktrain_data`). To disable pretrained word embeddings, set `embeddings=None` and randomly initialized word embeddings will be employed.   Additional pretrained embeddings such as those based on [BERT](https://arxiv.org/abs/1810.04805) or [ELMO](https://arxiv.org/abs/1802.05365) are expected to be included in future versions of *ktrain*.  Use of pretrained word embeddings will typically boost final accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Define a Model\n",
    "\n",
    "The `print_sequence_taggers` function shows that, as of this writing, *ktrain* currently supports a Bidirectional LSTM-CRM model for sequence tagging.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bilstm-crf: Bidirectional LSTM-CRF  (https://arxiv.org/abs/1603.01360)\n"
     ]
    }
   ],
   "source": [
    "text.print_sequence_taggers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained cbow word embeddings will be used with bilstm-crf\n",
      "Loading pretrained word vectors...this may take a few moments...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model = text.sequence_tagger('bilstm-crf', preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Train and Evaluate the Model\n",
    "\n",
    "Here, we will train for a single epoch using a learning rate of 0.001 (the default learning rate for Adam in Keras) and see how well we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1349/1349 [==============================] - 395s 293ms/step - loss: 3.5553 - val_loss: 3.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f251bc909e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our F1-score is **83.26** after a single pass through the dataset. Not bad for a single epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F1: 83.26\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      geo       0.86      0.88      0.87      3773\n",
      "      org       0.67      0.74      0.71      1935\n",
      "      tim       0.88      0.83      0.85      2058\n",
      "      art       0.00      0.00      0.00        35\n",
      "      per       0.79      0.80      0.80      1669\n",
      "      gpe       0.98      0.92      0.95      1558\n",
      "      nat       0.00      0.00      0.00        23\n",
      "      eve       0.20      0.05      0.08        20\n",
      "\n",
      "micro avg       0.83      0.83      0.83     11071\n",
      "macro avg       0.83      0.83      0.83     11071\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8326110509209101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate(class_names=preproc.get_classes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke `view_top_losses` to see the sentence we got the most wrong. This single sentence about James Brown contains 10 words that are misclassified.  We can see here that our model has trouble with titles of songs. In addition, some of the ground truth labels for this example are sketchy and incomplete, which also makes things difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total incorrect: 10\n",
      "Word            True : (Pred)\n",
      "==============================\n",
      "Mr.            :B-per (B-per)\n",
      "Brown          :I-per (I-per)\n",
      "is             :O     (O)\n",
      "known          :O     (O)\n",
      "by             :O     (O)\n",
      "millions       :O     (O)\n",
      "of             :O     (O)\n",
      "fans           :O     (O)\n",
      "as             :O     (O)\n",
      "\"              :O     (O)\n",
      "The            :O     (O)\n",
      "Godfather      :B-per (B-org)\n",
      "of             :O     (O)\n",
      "Soul           :B-per (B-per)\n",
      "\"              :O     (O)\n",
      "thanks         :O     (O)\n",
      "to             :O     (O)\n",
      "such           :O     (O)\n",
      "classic        :O     (O)\n",
      "songs          :O     (O)\n",
      "as             :O     (O)\n",
      "\"              :O     (O)\n",
      "Please         :B-art (O)\n",
      ",              :O     (O)\n",
      "Please         :O     (B-geo)\n",
      ",              :O     (O)\n",
      "Please         :O     (O)\n",
      ",              :O     (O)\n",
      "\"              :O     (O)\n",
      "\"              :O     (O)\n",
      "It             :O     (O)\n",
      "'s             :O     (O)\n",
      "a              :O     (O)\n",
      "Man            :O     (O)\n",
      "'s             :O     (O)\n",
      "World          :O     (O)\n",
      ",              :O     (O)\n",
      "\"              :O     (O)\n",
      "and            :O     (O)\n",
      "\"              :O     (O)\n",
      "Papa           :B-art (B-org)\n",
      "'s             :I-art (O)\n",
      "Got            :I-art (O)\n",
      "a              :I-art (O)\n",
      "Brand          :I-art (B-org)\n",
      "New            :I-art (I-org)\n",
      "Bag            :I-art (I-org)\n",
      ".              :O     (O)\n",
      "\"              :O     (O)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.view_top_losses(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on New Sentences\n",
    "\n",
    "Let's use our model to extract entities from new sentences. We begin by instantating a `Predictor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('As', 'O'),\n",
       " ('of', 'O'),\n",
       " ('2019', 'B-tim'),\n",
       " (',', 'O'),\n",
       " ('Donald', 'B-per'),\n",
       " ('Trump', 'I-per'),\n",
       " ('is', 'O'),\n",
       " ('still', 'O'),\n",
       " ('the', 'O'),\n",
       " ('President', 'B-per'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('United', 'B-geo'),\n",
       " ('States', 'I-geo'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('As of 2019, Donald Trump is still the President of the United States.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the predictor for later deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.save('/tmp/mypred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_predictor = ktrain.load_predictor('/tmp/mypred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paul', 'B-per'),\n",
       " ('Newman', 'I-per'),\n",
       " ('is', 'O'),\n",
       " ('my', 'O'),\n",
       " ('favorite', 'O'),\n",
       " ('American', 'B-gpe'),\n",
       " ('actor', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_predictor.predict('Paul Newman is my favorite American actor.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
